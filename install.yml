--- 
# file: install.yml
# This file is used for installing a container host 
   
# Not sure why, but I needed this to create a /etc/hosts that actually works with the Ubuntu image here on openstack
- hosts: master:worker 
  sudo: True
  # Idempotent way to build a /etc/hosts file with Ansible using your Ansible hosts inventory for a source.
  # Will include all hosts the playbook is run on.
  # Inspired from http://xmeblog.blogspot.com/2013/06/ansible-dynamicaly-update-etchosts.html
  tasks:
  - name: Build hosts file for all hosts
    lineinfile: dest=/etc/hosts regexp='.*{{ item }}$' line="{{ hostvars[item].ansible_default_ipv4.address }} {{item}}" state=present
    when: hostvars[item].ansible_default_ipv4.address is defined and docker is not defined
    with_items: groups['all']
  - name: Setting hostname to {{ inventory_hostname }}
    hostname: name={{ inventory_hostname }}
  - name: Disable automatic retrieval of hostname from dhcp on Amazon EC2
    template: src=roles/common/templates/dhclient.conf dest=/etc/dhcp/dhclient.conf
  - name: Disable setting hostname from /etc/hostname on Amazon EC2
    template: src=roles/common/templates/hostname.conf dest=/etc/init/hostname.conf
  - name: Disable ephemeral mount at /mnt for amazon AWS
    shell: umount /mnt
    register: umount_output
    ignore_errors: yes

  # This is a super-fugly way of determining the number of hosts in total, Ansible bug referenced
- hosts: master:worker
  sudo: True
  tasks:
  - name: Determine number of hosts in fugly way 
    include: roles/common/tasks/determine_num_hosts.yml
     
- hosts: master:worker 
  sudo: True
  vars:
      shared_storage_system: lvm
      vendor_data: /datastore
      lvm_device_whitelist: /dev/xvdd,/dev/xvde
  include: roles/storage/common/tasks/play_deploy_lvm_storage.yml
  when: single_node_lvm is defined

- hosts: master:worker
  sudo: True
  tasks:
  - name: Make sure ubuntu owns the datastore 
    file: path=/datastore owner=ubuntu group=ubuntu 


- hosts: master:worker 
  sudo: True
  vars:
   swap_on: False
  roles:
    - { role: mount_device, when: device is defined }
    # 9000 equals to aproximately 10 GB of SWAP; we don't want to run the playbook if the instance already has 10 GB of SWAP because the role was most probably already applied.
    - { role: swap-enabled, when: swap_on and ansible_swaptotal_mb < 9000 }
    - { role: java, java_provider: Cloudera }
    - { role: install-docker }
    - { role: seqware-container }
    - { role: workflow-dewrapper-dependencies, when: (workflow_name is defined and 'DEWrapper' in workflow_name ) }

- hosts: master 
  sudo: True
  vars:
    installed_workflows_path: /workflows
    target_workflow_path: /workflows
    user_name: ubuntu 
  roles:
    - { role: workflow, when: workflows is defined }

- hosts: master
  sudo: True
  roles:
    - { role: arch_3 }

- hosts: master:worker
  sudo: True
  vars:
    user_name: ubuntu
  tasks:
    - name: Copy pem keys to worker (Did you remember to replace the dummy gnos.pem with a valid pem key?)
      copy: src={{ item }} dest=/home/{{ user_name }}/.ssh/ owner={{ user_name }} group={{ user_name }}
      with_fileglob:
        - /home/{{ user_name }}/.ssh/*.pem

